To extract our part-of-speech (POS) features, we first tag the transcripts using the NLTK POS tagger (Bird et al., 2009). We use the POS of the current word, the next word, and the previous word as features.


One popular
way of framing the problem is to treat it as a
sequence tagging problem, where each interword
boundary must be labelled as either a sentence
boundary (B) or not (NB) (Liu and Shriberg, 2007).


Punctuation prediction can be treated as a sequence la-
belling task and tackled by CRF [3, 18]. Liu et al. [3] have
shown that a linear chain CRF yields a lower error rate than H-
MM and Maxent on the NIST sentence boundary detection task.
They owe the performance gain to CRF’s abilities to directly
estimate the posterior boundary label probabilities, to support

In the CRF, the conditional probability of an entire
label sequence given a feature sequence is modeled with
an exponential distribution.

Y. Liu, A. Stolcke, E. Shriberg, and M. Harper, “Using conditional
random fields for sentence boundary detection in speech,” in Pro-
ceedings of the 43rd Annual Meeting on Association for Compu-
tational Linguistics. Association for Computational Linguistics,
2005, pp. 451–458.


Neural networks provide a flexible architecture for con-
structing and synthesizing complex models. We take advantage
of this flexibility by training a punctuation restoration model in
two phases.

We attempt three different models for the training pro-
cess. The first model is a typical deep neural network
with 3 hidden fully connected layers. We address it as
DNN in our experiments. The m × n input matrix will
be reshaped into a long vector, which equals to concate-
nate the m word vectors together, and this long vector acts
as the input layer of the neural network. Then each train-
ing sample can be represented as {X i , Y i }, i = 1, 2, ... , N .
X i ∈ R m×n , X i = {x 1 , x 2 , ... , x m×n }, while Y i ∈ R K ,
y i = {y 1 , y 2 , ... , y K }.


In order to prevent co-adaptation between neurons in the
hidden layers, we implement “dropout” on fully connected
layers, which randomly “hide” some neurons along with
their connections to reduce overfitting (Srivastava et al.,
2014). Dropout is also applied in the fully connected layers
in other two models.

The purpose of this paper is to present our
method, DeepBond, for automatic sentence seg-
mentation of spontaneous speech of healthy el-
derly (CTL) and MCI patients. Although it was
evaluated for BP data, it can be adapted to other
languages as well.

Different from the aforementioned studies, the method
developed in this paper trains the model using a rich set
of both prosodic and lexical features. Besides, unlike the
way of integrating different kind of features in previous DT-
HMM [6], DT-CRF [21] and DNN-CRF [14] approaches,
our proposed method combines the prosodic and lexical fea-
tures at the beginning as the inputs of a single model without
individually modeling each category features. Our motiva-
tion is to learn the salient and complementary information
between the combined raw features for effectively discrim-
inating sentence boundary or non-boundary by the model
itself. Another difference is that a deep bidirectional LSTM
network is used to learn effective feature representations
and capture long term memory, so as to exploit the tem-
poral information. The structure of the deep bidirectional
LSTM network will circumvent the serious limitations of
shallow models or DNN using a fixed window size in pre-
vious studies. Our experiments show that differences lead
to significant improvement in sentence boundary detection
task.

Inspired by these experiments,
we would like to attempt this combination in the task of
punctuation prediction in this paper.

The pre-trained word
vectors are fed into proposed models based on DNN or
CNN for training.

We choose the 50-dimensional pre-trained word vec-
tors from GloVe for our experiments 1 . Compared with other publicly available word vector packs, such as 300-
dimensional word2vec 2 , GloVe-50d is a much smaller set,
not only dimensional, but also on the total words involved.
But for our purpose, GloVe-50d is already sufficient. And
in all our experiments, we will use 5-words subsequence as
a sample.

Conditional random fields (CRF) (Lafferty et al.,
2001) have been widely used in various sequence
labeling and segmentation tasks (Sha and Pereira,
003; Tseng et al., 2005). Unlike a HMM which
models the joint distribution of both the label se-
quence and the observation, a CRF is a discrimi-
native model of the conditional distribution of the
complete label sequence given the observation.
Specifically, a first-order linear-chain CRF which
assumes first-order Markov property is defined by
the following equation:

where x is the observation and y is the label se-
quence. Feature functions f k with time step t are
defined over the entire observation x and two adja-
cent hidden labels. Z(x) is a normalization factor to
ensure a well-formed probability distribution.


NLP applications generally receive text as input; therefore, words can be consid-
ered the basic processing unit. In this case, it is important that they are represented in
a way which carries the load of all relevant information. In the current approach used
here, words are induced representations in a dense vector space. These representations
are known as word embeddings; able to capture semantic, syntactic and morphological
information from large unannotated corpora [Mikolov et al. 2013, Ling et al. 2015, Lai
et al. 2015].

Even though textual information is
a strong indicator for sentence delimitation, boundaries are often associated with prosodic
information [Shriberg et al. 2000,Batista et al. 2012], like pause duration, change in pitch
and change in energy. However, the extraction of this type of information requires the use
of high quality resources, and consequently, few resources with prosodic information are
available.

Recently, the work of [Treviso et al. 2017] proposed an automatic SBD method
for impaired speech in Brazilian Portuguese, to allow a neuropsychological evaluation
based on discourse analysis. The method uses RCNNs (Recurrent Convolutional Neural
Networks) which independently treat prosodic and textual information, reaching state-of-
the-art results for impaired speech. Also, this study showed that it is possible to achieve
good results when comparing them with prepared speech, even when practically the same
quantity of text is used. Another interesting evidence was that the use of word embed-
dings, without morpho-syntactic labels was able to present the same results as when they
were used; this indicates that word embeddings contain sufficient morpho-syntactic in-
formation for SBD. It was also shown that the method gains the better results than the
state-of-the-art method used by [Fraser et al. 2015] by a great margin for both impaired
and prepared speech (an absolute difference of ∼0.20 and ∼0.30, respectively). Beyond
these findings, the method showed that the performance remains the same when a differ-
ent story is used.


A multilayer perceptron (MLP) is utilized with a nonlinear relu activation function and one hidden layer consisting of 128 hidden units.

We propose to use bidirectional LSTM 
(BLSTM) and deep network architecture to consider the both
past and future inputs as well as to model the complex rela-
tionships between input feature and output labels.
The bidirectional LSTM is proposed to use all available
input information in the past and future of a specific time
frame [43] by two parts: forward states, h t , and backward
states,
Furthermore, since context
information are useful for sequential labeling task, i.e.,
sentence boundary detection, the deep bidirectional LSTM
approach makes the decision for the input sequence by oper-
ating in both forward and backward directions to use the
history and future information.


As in Graves and Jaitly (2014), we used the
features by looking at forward states and back-
ward states. This kind of mechanism is known
as a bidirectional neural network (BRNN), since
it learns weights based on both past and future el-
ements given a timestep t. In order to implement
the BRNN, we reversed the sentences as a trick
before we fed them to a regular LSTM layer, dou-
bling the number of weights used in the recurrent
layer. The output from this layer is the summation
of the forward output with backward output:
−
y t = ←
y − t + →
y t
(6)
With a bidirectional LSTM layer, we are able
to explore the principle that words nearby have a
greater influence in classification, while consider-
ing that words farther away can also have some
impact.


@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}
